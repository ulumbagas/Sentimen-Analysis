{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "18cfnRmrCHNZL3zDV6Pzb3a7AM_flcil-",
      "authorship_tag": "ABX9TyNF7ZPfa9SO0ZQS17241C9h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ulumbagas/Sentimen-Analysis/blob/main/Alun-alun%20Jombang/sentimen_Alun_alun_Jombang.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analisis sentimen Alun-alun Jombang"
      ],
      "metadata": {
        "id": "I85-xBR54DmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import library"
      ],
      "metadata": {
        "id": "FONqjdKWTffL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#jangan pakai ulasan_clean_stopword coba pakai ulasan_clean\n",
        "!pip install indoNLP\n",
        "!pip install nlp-id"
      ],
      "metadata": {
        "id": "cPlTHC_eToTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from nlp_id.lemmatizer import Lemmatizer\n",
        "from nlp_id.stopword import StopWord\n",
        "from indoNLP.preprocessing import pipeline, replace_word_elongation, replace_slang\n",
        "from transformers import pipeline as hf_pipeline\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)"
      ],
      "metadata": {
        "id": "OoAldIN6TfAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ],
      "metadata": {
        "id": "U1mhme474Fmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_data='/content/drive/MyDrive/Hugging Face/alun-alun jombang/alun_alun_jombang_reviews_googlemaps.csv'\n",
        "df = pd.read_csv(path_data).drop_duplicates()\n",
        "print(\"Shape dataset:\", df.shape)"
      ],
      "metadata": {
        "id": "99saGAtc4VKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning & Preprocessing"
      ],
      "metadata": {
        "id": "m1L98znZmLru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalisasi"
      ],
      "metadata": {
        "id": "k9uuz2NZoSHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_review(text: str) -> str:\n",
        "    \"\"\"Lowercase, remove emoji/simbol non-ASCII, dan spasi berlebihan\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = text.encode(\"ascii\", \"ignore\").decode(\"ascii\")  # remove emoji/simbol\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", text)            # keep alphanumeric\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()"
      ],
      "metadata": {
        "id": "iZIk1B6n4JiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm = {\n",
        "    \" gk \": \" tidak \",\n",
        "    'aloon ':'alunalun',\n",
        "    \"jd \":\"jadi \",\n",
        "    \" ga \": \" tidak \",\n",
        "    \" gak \": \" tidak \",\n",
        "    \" g \":\" tidak \",\n",
        "    \" nggak \": \" tidak \",\n",
        "    \" jg \": \" juga \",\n",
        "    \" tp \": \" tapi \",\n",
        "    \" krn \": \" karena \",\n",
        "    \" sm \": \" sama \",\n",
        "    \" dg \": \" dengan \",\n",
        "    \" dgn \": \" dengan \",\n",
        "    \" aja \": \" saja \",\n",
        "    \" udh \": \" sudah \",\n",
        "    \" blm \": \" belum \",\n",
        "    \" skrg \": \" sekarang \",\n",
        "    ' umntuk':' untuk',\n",
        "    ' krg':' kurang',\n",
        "    \" trs \": \" terus \",\n",
        "    \" bgt \": \" banget \",\n",
        "    \" bnyk \": \" banyak \",\n",
        "    \" tmpt \": \" tempat \",\n",
        "    \" kl \": \" kalau \",\n",
        "    \" klo \": \" kalau \",\n",
        "    \" sy \": \" saya \",\n",
        "    \" aq \": \" saya \",\n",
        "    \" gue \": \" saya \",\n",
        "    \" gua \": \" saya \",\n",
        "    \" km \": \" kamu \",\n",
        "    \" lu \": \" kamu \",\n",
        "    \" ok \": \" baik \",\n",
        "    \" oke \": \" baik \",\n",
        "    \" sip \": \" baik \",\n",
        "    \" d \" : \" di \",\n",
        "    \"krn \":\"karena \",\n",
        "    \" rekomen \": \" rekomendasi \",\n",
        "    \" recommended \": \" rekomendasi \",\n",
        "    'many pigeons flying':'banyak merpati terbang',\n",
        "    \"alun alun\":\"alunalun\",\n",
        "    \"yg \": \"yang \",\n",
        "    \"skrg \": \"sekarang \",\n",
        "    \"alun2 \":\"alunalun \",\n",
        "    \"alon2\": \"alunalun\",\n",
        "    \"jalan2 \": \"jalan jalan \",\n",
        "    \"spot \":\"tempat \",\n",
        "    \"utk \":\"untuk \",\n",
        "    \"deket \":\"dekat \",\n",
        "    \"enk\": \"enak \",\n",
        "    \"public place\": \"tempat umum \",\n",
        "    \"dsb \": \"dan sebagainya \",\n",
        "    'rame ': 'ramai ',\n",
        "    'krg ': 'kurang ',\n",
        "    ' unt ': ' untuk ',\n",
        "    ' tdk ': ' tidak ',\n",
        "    'anak2 ':'anakanak ',\n",
        "    'anak-anak':'anakanak',\n",
        "    'anak anak': 'anakanak',\n",
        "    ' n ': ' dan ',\n",
        "    ' sampah2 ': ' sampah ',\n",
        "    ' dr ': ' dari ',\n",
        "    ' klo ': ' kalau ',\n",
        "    ' ayh ': ' ayah ',\n",
        "    ' dprsiapkan ': ' dipersiapkan ',\n",
        "    ' orang2 ':' orang orang ',\n",
        "    'sak jane ': ' sebenarnya ',\n",
        "    'percis ': 'persis ',\n",
        "    'sdh ': 'sudah ',\n",
        "    'org ': 'orang ',\n",
        "    'wkwk':'',\n",
        "    'wort it ':'cukup baik ',\n",
        "    'worth it ':'cukup baik ',\n",
        "    'poll':'',\n",
        "    'jbg':'jombang',\n",
        "    'cepet ':'cepat ',\n",
        "    'tmpat':'tempat',\n",
        "    'emg ' : 'memang ',\n",
        "    'bgt ' : 'sekali ',\n",
        "    'temen ': 'teman ',\n",
        "    'banget ': 'sekali  ',\n",
        "    'bnngeeett': 'sekali',\n",
        "    'seruuuuu': 'seru',\n",
        "    'bbrp':'beberapa',\n",
        "    'icon': 'ikon',\n",
        "    'happy': 'bahagia',\n",
        "    'overall': 'secara umum',\n",
        "    'love u': 'aku suka',\n",
        "    'weekend': 'akhir minggu',\n",
        "    'alon alon': 'alunalun',\n",
        "    'remang2':'remang remang',\n",
        "    'temen2':'teman teman',\n",
        "    'ngga ': 'tidak ',\n",
        "    'enak2': 'enak',\n",
        "    'kota2': 'kota',\n",
        "    'play ground':\"tempat bermain\",\n",
        "    'playgrond': \"tempat bermain\",\n",
        "    'play ground': \"tempat bermain\",\n",
        "    'playground':'tempat bermain',\n",
        "    'bareng': 'bersama',\n",
        "    'puaaanas': 'panas',\n",
        "    'nyangkruk': 'berkumpul',\n",
        "    'jombanng':'jombang',\n",
        "    'sore2':'sore',\n",
        "    ' jl ': ' jalan ',\n",
        "    'hangout': 'jalan-jalan bersama',\n",
        "    'laper ': 'lapar ',\n",
        "    'enggak ': 'tidak ',\n",
        "    ' city ':' kota ',\n",
        "    'ruame ':'ramai ',\n",
        "    'rame':'ramai',\n",
        "    'makananx ': 'makanannya ',\n",
        "    'free ':'gratis ',\n",
        "    'entrance ': 'masuk ',\n",
        "    'mkanan ': 'makanan ',\n",
        "    'bagus2':'bagus',\n",
        "    'alun alunnya':'alunalun',\n",
        "    'alun2nya':'alunalun',\n",
        "    'first waktu':'pertama kali',\n",
        "    'alun2nya':'alunalun',\n",
        "    'tpat ':'tempat ',\n",
        "    'cangtip':'cantik',\n",
        "    'indak ':'tidak ',\n",
        "    'kumpul2':'berkumpul',\n",
        "    'tyap ':'setiap ',\n",
        "    'alun alunya':'alunalun',\n",
        "    'taman2 ':'taman ',\n",
        "    'alunalunya ':'alunalun ',\n",
        "    'buangeeetttt':'sekali',\n",
        "    'panaaaaaaassss':'panas',\n",
        "    'santai2 ':'santai ',\n",
        "    'pagi2 ':'pagi ',\n",
        "    'nice place':'tempat bagus',\n",
        "    'jooos men':'bagus sekali',\n",
        "    'sebrang ':'seberang',\n",
        "    'nganter ':'mengantar ',\n",
        "    'apik ':'bagus ',\n",
        "    'car free day':'hari bebas kendaraan bermotor',\n",
        "    'alun2':'alunalun',\n",
        "    'momong':'mengasuh',\n",
        "    'asyk ':'asyik ',\n",
        "    'pusat2':'pusat',\n",
        "    'panas2 ':'panas ',\n",
        "    'jauh2 ':'jauh',\n",
        "    'teleknya ':'kotorannya ',\n",
        "    'hbos ': 'habis ',\n",
        "    'hlan halan ':'jalan jalan ',\n",
        "    'keceh ':'keren ',\n",
        "    'jomabng ':'jombang ',\n",
        "    'pisan ':'sekali ',\n",
        "    'unuk ':'untuk ',\n",
        "    ' mbuat ':' membuat ',\n",
        "    \" mjadi \":\" menjadi \",\n",
        "    'baguus': 'bagus',\n",
        "    'kids friendly':'ramah anak',\n",
        "    'child friendly':'ramah anak',\n",
        "    \" layan \":'',\n",
        "    'bocil':'anak',\n",
        "    'jualana':'jualan',\n",
        "    'banwa':'membawa',\n",
        "    'smpah ':'sampah ',\n",
        "    'smbarangan ':'sembarangan ',\n",
        "    'ngemong ':'mengasuh',\n",
        "    'playdtound':'taman bermain',\n",
        "    'playgorund ':'taman bermain ',\n",
        "    'malming ':'malam minggu ',\n",
        "    'moga ':'semoga ',\n",
        "    'nyantai ':'bersantai ',\n",
        "    'xlo ':'kalau ',\n",
        "    'quality time':'waktu yang berkualitas',\n",
        "    'berhati2': 'hati-hati',\n",
        "    'direnov ':'direnovasi',\n",
        "    'family':'keluarga',\n",
        "    'wig end':'akhir minggu',\n",
        "    'week end':'akhir minggu',\n",
        "    'nice ':'bagus ',\n",
        "    'rekomended ':'Direkomendasikan ',\n",
        "    'brtugas ':'bertugas ',\n",
        "    'kalo ':'kalau ',\n",
        "    'pulkam ':'pulang kampung '\n",
        "\n",
        "}\n",
        "\n",
        "def normalisai(text: str, norm_dict: dict) -> str:\n",
        "    for k, v in norm_dict.items():\n",
        "        text = text.replace(k, v)\n",
        "    return text"
      ],
      "metadata": {
        "id": "6460XQhAnufM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalisasi slang dari CSV GitHub\n",
        "slang_url='https://raw.githubusercontent.com/adeariniputri/text-preprocesing/master/slang.csv'\n",
        "\n",
        "slang = pd.read_csv(slang_url)\n",
        "slang_dict = dict(zip(\" \" + slang[\"slang\"] + \" \", \" \" + slang[\"formal\"] + \" \"))\n"
      ],
      "metadata": {
        "id": "wfkWvi9wHdzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word elongation & slang indoNLP\n",
        "def normalize_slang_elong(text: str) -> str:\n",
        "    pipe = pipeline([replace_word_elongation, replace_slang])\n",
        "    return pipe(text)"
      ],
      "metadata": {
        "id": "45qp2pLVMS2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================\n",
        "# Preprocessing Pipeline\n",
        "# =======================\n",
        "df[\"ulasan_clean\"] = (\n",
        "    df[\"ulasan\"]\n",
        "    .apply(clean_review)\n",
        "    .apply(lambda x: normalisai(x, norm))\n",
        "    .apply(lambda x: normalisai(x, slang_dict))\n",
        "    .apply(normalize_slang_elong)\n",
        ")"
      ],
      "metadata": {
        "id": "eMl8JyWz5jv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#random cek\n",
        "import random\n",
        "random_number = random.randint(1, len(df))\n",
        "df[['ulasan','ulasan_clean']][random_number:(random_number+50)]"
      ],
      "metadata": {
        "id": "YfPt9-jy8jaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Steaming / lemmatizer"
      ],
      "metadata": {
        "id": "LftIT4b6YPe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "lemmatizer = Lemmatizer()\n",
        "def stemming(text: str) -> str:\n",
        "    return lemmatizer.lemmatize(text)"
      ],
      "metadata": {
        "id": "4Iir2N4G1e29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop word"
      ],
      "metadata": {
        "id": "G-jClBvQ9nvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopword = StopWord()\n",
        "stopwords_list = stopword.get_stopword()\n",
        "hapus_stopword =['satu']\n",
        "for kata in hapus_stopword:\n",
        "    stopwords_list.remove(kata)\n",
        "\n",
        "def remove_stopwords(text: str) -> str:\n",
        "  tokens=text.split()\n",
        "  tokens=[word for word in tokens if word not in stopwords_list]\n",
        "  return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "ZqfsRZdQ1eq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"ulasan_clean_stopword\"] = df[\"ulasan_clean\"].apply(remove_stopwords)\n",
        "df[\"ulasan_stemming\"] = df[\"ulasan_clean_stopword\"].apply(stemming)\n"
      ],
      "metadata": {
        "id": "I1McnPIK9wlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentimen"
      ],
      "metadata": {
        "id": "CCd6-CyU-tfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## w11wo model"
      ],
      "metadata": {
        "id": "tk_YSW1Yb3u7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "pretrained_name = \"w11wo/indonesian-roberta-base-sentiment-classifier\"\n",
        "\n",
        "sentimen_w11wo = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=pretrained_name,\n",
        "    tokenizer=pretrained_name\n",
        ")"
      ],
      "metadata": {
        "id": "KGy5cs0jo04V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def w11wo_sentimen(text: str) -> str:\n",
        "  if not isinstance(text, str) or text.strip() == \"\":\n",
        "        return \"unknown\"\n",
        "  sentimen=sentimen_w11wo(text[:512])[0]\n",
        "  return sentimen[\"label\"]\n"
      ],
      "metadata": {
        "id": "qtTrU98_gNJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['w11wo_model_sentimen']=df['ulasan_stemming'].apply(w11wo_sentimen)\n",
        "df['w11wo_model_sentimen'].value_counts()"
      ],
      "metadata": {
        "id": "MjR6wV-pgkQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "8eQiGiYWmY9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath='/content/drive/MyDrive/Hugging Face/alun-alun jombang/review_clean.csv'\n",
        "\n",
        "df.to_csv(filepath,index=False)"
      ],
      "metadata": {
        "id": "axlakFpmTuFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2058448"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "from matplotlib import pyplot as plt\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk import ngrams\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sentiment=df[['ulasan_clean','ulasan_stemming','w11wo_model_sentimen']]\n",
        "df_sentiment['ulasan_stemming'] = df_sentiment['ulasan_stemming'].str.replace('anakanak', 'anak-anak')\n",
        "df_sentiment['ulasan_stemming'] = df_sentiment['ulasan_stemming'].str.replace('anakanaknya', 'anak-anaknya')\n",
        "df_sentiment['ulasan_stemming'] = df_sentiment['ulasan_stemming'].str.replace('alunalun', 'alun-alun')\n",
        "df_sentiment.head()"
      ],
      "metadata": {
        "id": "DztYwXgryPaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_reviews_by_sentiment(df,table:str,table_clean:str ,sentimen: str):\n",
        "    return df[df[table] == sentimen][table_clean]"
      ],
      "metadata": {
        "id": "2Z1RVTNnjjVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentimen_labels = df['w11wo_model_sentimen'].unique()\n",
        "reviews_by_sentiment = {\n",
        "    sentimen: get_reviews_by_sentiment(df_sentiment,'w11wo_model_sentimen','ulasan_stemming', sentimen)\n",
        "    for sentimen in sentimen_labels\n",
        "}\n",
        "\n",
        "\n",
        "positif_review = reviews_by_sentiment['positive']\n",
        "negatif_review = reviews_by_sentiment['negative']\n",
        "neutral_review = reviews_by_sentiment['neutral']"
      ],
      "metadata": {
        "id": "gXfzqeopjjTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_text = \" \".join(str(review) for review in positif_review)\n",
        "negatif_text = \" \".join(str(review) for review in negatif_review)\n",
        "neutral_text = \" \".join(str(review) for review in neutral_review)"
      ],
      "metadata": {
        "id": "j_UtshrpjjRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_wordcloud(data, title):\n",
        "    cloud = WordCloud(width=1000,\n",
        "                      height=400,\n",
        "                      max_words=500,\n",
        "                      colormap='viridis',\n",
        "                      background_color='white',\n",
        "                      collocations=False\n",
        "\n",
        "                      ).generate_from_text(data)\n",
        "    plt.figure(figsize=(10,8))\n",
        "    plt.imshow(cloud)\n",
        "    plt.axis('off')\n",
        "    plt.title(title, fontsize=13)\n",
        "    plt.show()\n",
        "\n",
        "def word_freq(data, title):\n",
        "    data = data.split()\n",
        "\n",
        "    word_freq = Counter(data)\n",
        "    # ubah jadi DataFrame agar mudah dianalisis\n",
        "    freq_df = pd.DataFrame(word_freq.items(), columns=[\"kata\", \"jumlah\"]).sort_values(by=\"jumlah\", ascending=False)\n",
        "\n",
        "    # tampilkan 10 kata paling sering\n",
        "    top_n = 5\n",
        "    top_words = freq_df.head(top_n)\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.barh(top_words[\"kata\"], top_words[\"jumlah\"], color=\"skyblue\")\n",
        "    plt.gca().invert_yaxis()  # supaya urutan terbesar di atas\n",
        "    plt.title(f\"{top_n} Kata Paling Sering Muncul dalam Review {title}\")\n",
        "    plt.xlabel(\"Jumlah Kemunculan\")\n",
        "    plt.ylabel(\"Kata\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "VtLi4gNrjjOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dilihat perkata sentimen negatif dan positif terkesan sama, mari kita lihat 3 kata (trigram) -->chat gpt\n",
        "\n",
        "generate_wordcloud(positive_text,'Wordcloud Positif')\n",
        "\n",
        "print('')\n",
        "\n",
        "word_freq(positive_text,'Positif')"
      ],
      "metadata": {
        "id": "EUfzUeQejjLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_wordcloud(negatif_text,'Wordcloud negatif')\n",
        "print('')\n",
        "word_freq(negatif_text,'negatif')"
      ],
      "metadata": {
        "id": "uoHjRAHqjxh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_wordcloud(neutral_text,'Wordcloud neutral')"
      ],
      "metadata": {
        "id": "u2lidZKbjxfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def n_gram(text_data, n): # Modified to accept text_data as input\n",
        "    n_grams = ngrams(text_data.split(), n) # Use text_data instead of global text\n",
        "    return n_grams"
      ],
      "metadata": {
        "id": "2xbXYwatjxdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "df_frequency = []\n",
        "# Function frequency: counts the frequency of the each \"n-gram\" output\n",
        "# Parameter grammed: it takes the function \"n_gram\"s return as value. Briefly, the grammed text.\n",
        "def frequency(grammed):\n",
        "    sentences.clear()\n",
        "    df_frequency.clear()\n",
        "    freq = nltk.FreqDist(grammed)\n",
        "    for k, v in freq.items():\n",
        "        sentences.append(k)                     # Sentences is a list, stores the grams(ignores duplicates)\n",
        "        df_frequency.append(v)                  # df_frequency is a list, stores the frequency of grams"
      ],
      "metadata": {
        "id": "loxy-xovjxak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function create_plot: it creates a plot for given grams\n",
        "# Parameter num: it's a number to send it to \"n_gram\" function\n",
        "# Parameter text_data: the combined text data to analyze\n",
        "def create_plot(num, text_data): # Modified to accept text_data\n",
        "    frequency(n_gram(text_data, num)) # Send text_data and num parameter to \"n_gram func.\" and send the result to \"frequency func.\"\n",
        "\n",
        "    gram_frame = pd.DataFrame(sentences)       # gram_frame is the data frame to store grams and freq.\n",
        "\n",
        "    gram_frame['frequencies'] = df_frequency\n",
        "    if num == 2:\n",
        "        gram_frame.columns = ['first', 'second', 'frequencies']\n",
        "    if num == 3:\n",
        "        gram_frame.columns = ['first', 'second', 'third', 'frequencies']\n",
        "\n",
        "    gram_frame.sort_values(\"frequencies\", axis=0, ascending=False, inplace=True, na_position='last')\n",
        "\n",
        "    gram_frame = gram_frame.head(20)            # Only take the top 20 of gram_frame\n",
        "\n",
        "    total = sum(df_frequency)\n",
        "\n",
        "    gram_frame[\"ratio\"] = gram_frame['frequencies'].div(total)   # Additional, ratio is added\n",
        "\n",
        "    plt.rcdefaults()\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    if num == 2:\n",
        "        grams = gram_frame[\"first\"] + \" \" + gram_frame[\"second\"]\n",
        "    if num == 3:\n",
        "        grams = gram_frame[\"first\"] + \" \" + gram_frame[\"second\"] + \" \" + gram_frame[\"third\"]\n",
        "\n",
        "    # Create plot\n",
        "    y_pos = np.arange(len(grams))\n",
        "    performance = gram_frame[\"frequencies\"]\n",
        "\n",
        "    ax.barh(y_pos, performance)\n",
        "    ax.set_yticks(y_pos)\n",
        "    ax.set_yticklabels(grams)\n",
        "    ax.invert_yaxis()  # labels read top-to-bottom\n",
        "    ax.set_xlabel('Frequency')\n",
        "    ax.set_title(f'{num}-grams') # Changed title to be more informative\n",
        "\n",
        "    plt.show()\n",
        "    display(gram_frame)"
      ],
      "metadata": {
        "id": "JjyG8VBejxYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_plot(3, positive_text) # Pass positive_text to create_plot"
      ],
      "metadata": {
        "id": "C7ZhjnpWjxVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_plot(3, negatif_text)"
      ],
      "metadata": {
        "id": "TFNNf5bgjxTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalasi BERTopic\n",
        "!pip install bertopic\n",
        "\n",
        "# Instalasi sentence-transformers (jika belum ada)\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "8sy9CcZvjxQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "embedding_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "topic_model = BERTopic(embedding_model=embedding_model,language='indonesian')\n",
        "topics, probs = topic_model.fit_transform(df_sentiment['ulasan_stemming'])"
      ],
      "metadata": {
        "id": "Yn-msJrZjxOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_info = topic_model.get_topic_info()\n",
        "topic_info.head()"
      ],
      "metadata": {
        "id": "hndFrGl1jxL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_info['Representation'][:8]"
      ],
      "metadata": {
        "id": "DGjGQfHrjxJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aspects=['area','main','jombang','alunalun','fasilitas','parkir','keluarga','sampah','bersih','tempat','suasana','lokasi','kota']"
      ],
      "metadata": {
        "id": "fVrRwlJ9jjEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"yangheng/deberta-v3-base-absa-v1.1\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"yangheng/deberta-v3-base-absa-v1.1\")\n",
        "\n",
        "from transformers import pipeline\n",
        "pipe = pipeline(\"text-classification\", model=model,tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "prlboySrmBTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_aspects(text):\n",
        "    found = [a for a in aspects if a in text.lower()]\n",
        "    return found if found else []\n"
      ],
      "metadata": {
        "id": "qZFm1sjpkO6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_aspect_sentiment(text):\n",
        "    aspek_ditemukan = extract_aspects(text)\n",
        "    hasil = []\n",
        "\n",
        "    for asp in aspek_ditemukan:\n",
        "        sent = pipe(text, text_pair=asp)\n",
        "        hasil.append({\n",
        "            \"aspect\": asp,\n",
        "            \"sentiment\": sent[0][\"label\"]\n",
        "        })\n",
        "    return hasil\n"
      ],
      "metadata": {
        "id": "eVHBYcVKkO4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "df_sentiment[\"aspect_sentiment\"] = df_sentiment[\"ulasan_stemming\"].progress_apply(get_aspect_sentiment)\n"
      ],
      "metadata": {
        "id": "YbsHoF_IkO1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sentiment.head()"
      ],
      "metadata": {
        "id": "8PNlmuqXkOye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# expand hasil analisis jadi baris terpisah\n",
        "rows = []\n",
        "for i, row in df_sentiment.iterrows():\n",
        "    for asp in row[\"aspect_sentiment\"]:\n",
        "        rows.append({\n",
        "            \"ulasan_clean\": row[\"ulasan_clean\"],\n",
        "            \"ulasan_stemming\": row[\"ulasan_stemming\"],\n",
        "            \"w11wo_model_sentimen\": row[\"w11wo_model_sentimen\"],\n",
        "            \"aspect\": asp[\"aspect\"],\n",
        "            \"sentiment\": asp[\"sentiment\"]\n",
        "        })\n",
        "\n",
        "df_result = pd.DataFrame(rows)\n",
        "\n"
      ],
      "metadata": {
        "id": "70181BBLkOwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_result['ulasan_clean'] = df_result['ulasan_clean'].str.replace('anakanak', 'anak-anak')\n",
        "df_result['ulasan_clean'] = df_result['ulasan_clean'].str.replace('alunalun', 'alun-alun')"
      ],
      "metadata": {
        "id": "_Ivg-zDftGAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_result.iloc[1145:1165]"
      ],
      "metadata": {
        "id": "lytwSFvnso_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "insight = df_result.groupby([\"aspect\", \"sentiment\"]).size().reset_index(name=\"count\")\n",
        "insight.sort_values([\"aspect\", \"count\"], ascending=[True, False])\n"
      ],
      "metadata": {
        "id": "lFWFPcPBkOto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HmT93mdFkOrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PdLrmEXTkOoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tUjcLFxBkOmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K4w7uobqkOjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3dvZDzYvkOg_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}